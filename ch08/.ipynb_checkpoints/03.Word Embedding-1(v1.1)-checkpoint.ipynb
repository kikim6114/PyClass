{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:2.5em\">3. ë‹¨ì–´ ì„ë² ë”©(Word Embedding)-1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUTennVh9hdS"
   },
   "source": [
    "ì´ ìë£ŒëŠ” ë‹¤ìŒ ìë£Œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •ëœ ìë£Œì…ë‹ˆë‹¤.<br>\n",
    "- [ìœ„í‚¤ë…ìŠ¤ ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸, 04. ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ ë‹¨ì–´ í‘œí˜„(Count based word Representation)](https://wikidocs.net/22650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BASJDfjH9fsV",
    "outputId": "aa33f1cd-de7d-4193-b65b-03dbf97e4048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\kikim\\anaconda3\\envs\\nlp\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\kikim\\anaconda3\\envs\\nlp\\lib\\site-packages (from konlpy) (1.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\kikim\\anaconda3\\envs\\nlp\\lib\\site-packages (from konlpy) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\kikim\\anaconda3\\envs\\nlp\\lib\\site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\kikim\\anaconda3\\envs\\nlp\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (24.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vd2dXWscbAm_",
    "outputId": "c9a0a9ec-a04f-4ca0-86d8-f2f4ac74b7d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kikim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO3axRbLZ1xN"
   },
   "source": [
    "### BoW ì˜ˆ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oY2EfmYH9mTC"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "F-gc_IuKaAvt"
   },
   "outputs": [],
   "source": [
    "def build_bag_of_words(document):\n",
    "    # ë§ˆì¹¨í‘œ(.) ì œê±°\n",
    "    document = document.replace('.', '')\n",
    "    tokenized_document = okt.morphs(document)\n",
    "\n",
    "    word_to_index = {}\n",
    "    bow = []\n",
    "\n",
    "    for word in tokenized_document:  \n",
    "        if word not in word_to_index.keys():\n",
    "            word_to_index[word] = len(word_to_index)  \n",
    "            # BoWì— ì „ë¶€ ê¸°ë³¸ê°’ 1ì„ ë„£ëŠ”ë‹¤.\n",
    "            bow.insert(len(word_to_index) - 1, 1)\n",
    "        else:\n",
    "            # ì¬ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤\n",
    "            index = word_to_index.get(word)\n",
    "            # ì¬ë“±ì¥í•œ ë‹¨ì–´ëŠ” í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì˜ ìœ„ì¹˜ì— 1ì„ ë”í•œë‹¤.\n",
    "            bow[index] = bow[index] + 1\n",
    "\n",
    "    return word_to_index, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MMBgQyR19p4Z"
   },
   "outputs": [],
   "source": [
    "doc1 = \"ì •ë¶€ê°€ ë°œí‘œí•˜ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ê³¼ ì†Œë¹„ìê°€ ëŠë¼ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ì€ ë‹¤ë¥´ë‹¤.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NYYi4tLCaIKp"
   },
   "outputs": [],
   "source": [
    "vocab, bow = build_bag_of_words(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjDoSIrQAAyx",
    "outputId": "c7e59e8e-e80e-4a60-a95a-37e531943d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : {'ì •ë¶€': 0, 'ê°€': 1, 'ë°œí‘œ': 2, 'í•˜ëŠ”': 3, 'ë¬¼ê°€ìƒìŠ¹ë¥ ': 4, 'ê³¼': 5, 'ì†Œë¹„ì': 6, 'ëŠë¼ëŠ”': 7, 'ì€': 8, 'ë‹¤ë¥´ë‹¤': 9}\n",
      "Bag of Words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary :', vocab)\n",
    "print('Bag of Words vector :', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `bow` ì¶œë ¥ì—ì„œ 2ê°€ ë‚˜ì˜¨ ë¶€ë¶„ë“¤ì„ ì£¼ëª©í•´ ë³´ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pSeBvAxZ20A"
   },
   "source": [
    "### ë‹¤ë¥¸ BoW ì˜ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "GkDasWhbZ4rV"
   },
   "outputs": [],
   "source": [
    "doc2 = 'ì†Œë¹„ìëŠ” ì£¼ë¡œ ì†Œë¹„í•˜ëŠ” ìƒí’ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¼ê°€ìƒìŠ¹ë¥ ì„ ëŠë‚€ë‹¤.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gQuSPgXZ-n3",
    "outputId": "87f7e968-0129-499c-e6e7-0c6e290b37ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : {'ì†Œë¹„ì': 0, 'ëŠ”': 1, 'ì£¼ë¡œ': 2, 'ì†Œë¹„': 3, 'í•˜ëŠ”': 4, 'ìƒí’ˆ': 5, 'ì„': 6, 'ê¸°ì¤€': 7, 'ìœ¼ë¡œ': 8, 'ë¬¼ê°€ìƒìŠ¹ë¥ ': 9, 'ëŠë‚€ë‹¤': 10}\n",
      "Bag of Words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "vocab, bow = build_bag_of_words(doc2)\n",
    "print('Vocabulary :', vocab)\n",
    "print('Bag of Words vector :', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmbXkKVGaTq2",
    "outputId": "7cb26b77-c076-450b-e1d9-a2f88d04820d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •ë¶€ê°€ ë°œí‘œí•˜ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ê³¼ ì†Œë¹„ìê°€ ëŠë¼ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ì€ ë‹¤ë¥´ë‹¤. ì†Œë¹„ìëŠ” ì£¼ë¡œ ì†Œë¹„í•˜ëŠ” ìƒí’ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¼ê°€ìƒìŠ¹ë¥ ì„ ëŠë‚€ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "doc3 = doc1 + ' ' + doc2\n",
    "print(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCBLsvTtaarG",
    "outputId": "7897cd3c-dbfa-4bbd-f212-01a9e864cf91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : {'ì •ë¶€': 0, 'ê°€': 1, 'ë°œí‘œ': 2, 'í•˜ëŠ”': 3, 'ë¬¼ê°€ìƒìŠ¹ë¥ ': 4, 'ê³¼': 5, 'ì†Œë¹„ì': 6, 'ëŠë¼ëŠ”': 7, 'ì€': 8, 'ë‹¤ë¥´ë‹¤': 9, 'ëŠ”': 10, 'ì£¼ë¡œ': 11, 'ì†Œë¹„': 12, 'ìƒí’ˆ': 13, 'ì„': 14, 'ê¸°ì¤€': 15, 'ìœ¼ë¡œ': 16, 'ëŠë‚€ë‹¤': 17}\n",
      "Bag of Words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "vocab, bow = build_bag_of_words(doc3)\n",
    "print('Vocabulary :', vocab)\n",
    "print('Bag of Words vector :', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doc3ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì–´íœ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ doc1, doc2ì˜ BoWë¥¼ êµ¬í•œë‹¤ë©´, ì¶œë ¥ì€ ë‹¤ìŒê³¼ ê°™ì„ ê²ƒì´ë‹¤.\n",
    "> ë¬¸ì„œ3 ë‹¨ì–´ ì§‘í•©ì— ëŒ€í•œ ë¬¸ì„œ1 BoW : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
    "> ë¬¸ì„œ3 ë‹¨ì–´ ì§‘í•©ì— ëŒ€í•œ ë¬¸ì„œ2 BoW : [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: crimson\">[ì½”ë”©ğŸ]</span> ìœ„ì™€ ê°™ì€ ì¶œë ¥ì´ ë‚˜ì˜¤ë„ë¡ `build_bag_of_words` ë¥¼ ìˆ˜ì •í•´ë³´ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZUrPQoKarJi"
   },
   "source": [
    "### CountVectorizer í´ë˜ìŠ¤ë¡œ BoW ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J98w_wnAasQ0",
    "outputId": "a488e9b9-7612-478e-d87b-54c7ff30a354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 2 1 2 1]]\n",
      "vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# ì½”í¼ìŠ¤ë¡œë¶€í„° ê° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê¸°ë¡\n",
    "print('bag of words vector :', vector.fit_transform(corpus).toarray()) \n",
    "\n",
    "# ê° ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ê°€ ì–´ë–»ê²Œ ë¶€ì—¬ë˜ì—ˆëŠ”ì§€ë¥¼ ì¶œë ¥\n",
    "print('vocabulary :',vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary ë³´ê¸°ê°€ ì˜~ ë¶ˆí¸í•´!\n",
    "#### ì–´íœ˜ ì‚¬ì „ì„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë¦¬í•´ì„œ ì¶œë ¥/ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer ê°ì²´ì—ì„œ ì–´íœ˜ ë”•ì…”ë„ˆë¦¬ë¥¼ valueë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ë°˜í™˜\n",
    "def sorted_vocab(vect):\n",
    "    return dict(sorted(vect.vocabulary_.items(), key=lambda x: x[1], reverse=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'because': 0, 'know': 1, 'love': 2, 'want': 3, 'you': 4, 'your': 5}\n"
     ]
    }
   ],
   "source": [
    "word2idx = sorted_vocab(vector)\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë‹¨ì–´ `I` ê°€ ì—†ì–´ì§„ ì´ìœ : CountVectorizerê°€ ê¸°ë³¸ì ìœ¼ë¡œ ê¸¸ì´ê°€ 2ì´ìƒì¸ ë¬¸ìì— ëŒ€í•´ì„œë§Œ í† í°ìœ¼ë¡œ ì¸ì‹í•˜ê¸° ë•Œë¬¸\n",
    "- CountVectorizerê°€ ë„ì–´ì“°ê¸°ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ í† í°í™”í•˜ê³  BoWë¥¼ ë§Œë“ ë‹¤. ë”°ë¼ì„œ ì˜ì–´ì—ì„œëŠ” í° ë¬¸ì œê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë‚˜, í•œêµ­ì–´ì—ì„œëŠ” í’ˆì§ˆì´ ë§ì´ ë–¨ì–´ì§„ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "vocabulary : {'ê¸°ì¤€ìœ¼ë¡œ': 0, 'ëŠë¼ëŠ”': 1, 'ëŠë‚€ë‹¤': 2, 'ë‹¤ë¥´ë‹¤': 3, 'ë¬¼ê°€ìƒìŠ¹ë¥ ê³¼': 4, 'ë¬¼ê°€ìƒìŠ¹ë¥ ì€': 5, 'ë¬¼ê°€ìƒìŠ¹ë¥ ì„': 6, 'ë°œí‘œí•˜ëŠ”': 7, 'ìƒí’ˆì„': 8, 'ì†Œë¹„ìê°€': 9, 'ì†Œë¹„ìëŠ”': 10, 'ì†Œë¹„í•˜ëŠ”': 11, 'ì •ë¶€ê°€': 12, 'ì£¼ë¡œ': 13}\n"
     ]
    }
   ],
   "source": [
    "# doc3ë¡œ í•´ë³´ì.\n",
    "print('bag of words vector :', vector.fit_transform([doc3]).toarray()) \n",
    "print('vocabulary :',sorted_vocab(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'ë¬¼ê°€ìƒìŠ¹ë¥ ê³¼'ê³¼ 'ë¬¼ê°€ìƒìŠ¹ë¥ ì„'ì´ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì²˜ë¦¬ë¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShtX64wSa4H1"
   },
   "source": [
    "### ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œ BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jZlfTRh7a8FN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) ì‚¬ìš©ìê°€ ì§ì ‘ ì •ì˜í•œ ë¶ˆìš©ì–´ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NubeSc-Qa50l",
    "outputId": "9f7a3b5c-aa8f-489a-81fb-8a71a057a4db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1 1]]\n",
      "vocabulary : {'everything': 0, 'family': 1, 'important': 2, 'it': 3, 'thing': 4}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',sorted_vocab(vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) CountVectorizerì—ì„œ ì œê³µí•˜ëŠ” ìì²´ ë¶ˆìš©ì–´ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgwJWfn_a6qe",
    "outputId": "bf04a354-5ba3-4447-aeab-832f1700c854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1]]\n",
      "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',sorted_vocab(vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) NLTKì—ì„œ ì§€ì›í•˜ëŠ” ë¶ˆìš©ì–´ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Up_s0AZFa9R-",
    "outputId": "2f549759-6c1b-472e-cb1c-fdc8707063e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1]]\n",
      "vocabulary : {'everything': 0, 'family': 1, 'important': 2, 'thing': 3}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words=stop_words)\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray()) \n",
    "print('vocabulary :',sorted_vocab(vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒì´ì¬ìœ¼ë¡œ TF-IDF ì§ì ‘ êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¨ì–´ì¥ì˜ í¬ê¸° : 9\n",
      "['ê³¼ì¼ì´', 'ê¸¸ê³ ', 'ë…¸ë€', 'ë¨¹ê³ ', 'ë°”ë‚˜ë‚˜', 'ì‚¬ê³¼', 'ì‹¶ì€', 'ì €ëŠ”', 'ì¢‹ì•„ìš”']\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import pandas as pd\n",
    " \n",
    "docs = [\n",
    "  'ë¨¹ê³  ì‹¶ì€ ì‚¬ê³¼',\n",
    "  'ë¨¹ê³  ì‹¶ì€ ë°”ë‚˜ë‚˜',\n",
    "  'ê¸¸ê³  ë…¸ë€ ë°”ë‚˜ë‚˜ ë°”ë‚˜ë‚˜',\n",
    "  'ì €ëŠ” ê³¼ì¼ì´ ì¢‹ì•„ìš”'\n",
    "] \n",
    " \n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()\n",
    "print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :', len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs) # ë¬¸ì„œì˜ ì´ìˆ˜\n",
    " \n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(N/(df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ê³¼ì¼ì´</th>\n",
       "      <th>ê¸¸ê³ </th>\n",
       "      <th>ë…¸ë€</th>\n",
       "      <th>ë¨¹ê³ </th>\n",
       "      <th>ë°”ë‚˜ë‚˜</th>\n",
       "      <th>ì‚¬ê³¼</th>\n",
       "      <th>ì‹¶ì€</th>\n",
       "      <th>ì €ëŠ”</th>\n",
       "      <th>ì¢‹ì•„ìš”</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ê³¼ì¼ì´  ê¸¸ê³   ë…¸ë€  ë¨¹ê³   ë°”ë‚˜ë‚˜  ì‚¬ê³¼  ì‹¶ì€  ì €ëŠ”  ì¢‹ì•„ìš”\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "# ê° ë¬¸ì„œì— ëŒ€í•´ì„œ ì•„ë˜ ì—°ì‚°ì„ ë°˜ë³µ\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tf(t, d))\n",
    "        \n",
    "tf_ = pd.DataFrame(result, columns = vocab)  # í•¨ìˆ˜ëª… tf ì™€ êµ¬ë¶„í•˜ê¸° ìœ„í•´ underscore ì‚¬ìš©\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ê³¼ì¼ì´</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ê¸¸ê³ </th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ë…¸ë€</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ë¨¹ê³ </th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ë°”ë‚˜ë‚˜</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì‚¬ê³¼</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì‹¶ì€</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì €ëŠ”</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì¢‹ì•„ìš”</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "ê³¼ì¼ì´  0.693147\n",
       "ê¸¸ê³    0.693147\n",
       "ë…¸ë€   0.693147\n",
       "ë¨¹ê³    0.287682\n",
       "ë°”ë‚˜ë‚˜  0.287682\n",
       "ì‚¬ê³¼   0.693147\n",
       "ì‹¶ì€   0.287682\n",
       "ì €ëŠ”   0.693147\n",
       "ì¢‹ì•„ìš”  0.693147"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ê³¼ì¼ì´</th>\n",
       "      <th>ê¸¸ê³ </th>\n",
       "      <th>ë…¸ë€</th>\n",
       "      <th>ë¨¹ê³ </th>\n",
       "      <th>ë°”ë‚˜ë‚˜</th>\n",
       "      <th>ì‚¬ê³¼</th>\n",
       "      <th>ì‹¶ì€</th>\n",
       "      <th>ì €ëŠ”</th>\n",
       "      <th>ì¢‹ì•„ìš”</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ê³¼ì¼ì´        ê¸¸ê³         ë…¸ë€        ë¨¹ê³        ë°”ë‚˜ë‚˜        ì‚¬ê³¼        ì‹¶ì€  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         ì €ëŠ”       ì¢‹ì•„ìš”  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‚¬ì´í‚·ëŸ°ì„ ì´ìš©í•œ DTMê³¼ TF-IDF ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "vocabulary : {'do': 0, 'know': 1, 'like': 2, 'love': 3, 'should': 4, 'want': 5, 'what': 6, 'you': 7, 'your': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# ì½”í¼ìŠ¤ë¡œë¶€í„° ê° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê¸°ë¡\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# ê° ë‹¨ì–´ì™€ ë§µí•‘ëœ ì¸ë±ìŠ¤ ì¶œë ¥\n",
    "print('vocabulary :',sorted_vocab(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'do': 0, 'know': 1, 'like': 2, 'love': 3, 'should': 4, 'want': 5, 'what': 6, 'you': 7, 'your': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "# print(tfidfv.vocabulary_)\n",
    "print(sorted_vocab(tfidfv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "bag of words.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
